{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright 2018-2019 IBM Corp. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR - Visualize Results\n",
    "\n",
    "This example illustrates the usage of [MAX OCR](https://developer.ibm.com/exchanges/models/all/max-ocr) model. This notebook guides you through running the model on a sample image to get the text.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "The notebook calls the `MAX OCR` microservice, which must be running. You can either use the [hosted demo instance](http://max-ocr.codait-prod-41208c73af8fca213512856c7a09db52-0000.us-east.containers.appdomain.cloud), or follow the instructions for [deploying the microservice locally from the Dockerhub image](https://github.com/IBM/MAX-OCR#2-deploy-the-model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook requires matplotlib, Pillow and requests\n",
    "# You only need to run the line below to install these if you don't already have them installed\n",
    "\n",
    "! pip install -q matplotlib Pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This url must point to a running instance of the model microservice\n",
    "# By default this assumes you are using the hosted demo instance\n",
    "# If you want to use the model that is running locally, pass the `local_port` field.\n",
    "\n",
    "\n",
    "def call_model(input_img, local_port=None):\n",
    "    \"\"\"\n",
    "    Takes in input image file path, posts the image to the model and returns face bboxes and emotion predictions\n",
    "    If local port is not specified, uses the long running instance.\n",
    "    If local port is specified, uses the local instance.\n",
    "    \"\"\"\n",
    "    if local_port:\n",
    "        url = 'http://localhost:'+ str(local_port)+'/model/predict'\n",
    "    else:\n",
    "        url = 'http://max-ocr.codait-prod-41208c73af8fca213512856c7a09db52-0000.us-east.containers.appdomain.cloud/model/predict'\n",
    "\n",
    "    files = {'image': ('image.jpg', open(input_img, 'rb'), 'images/jpeg') }\n",
    "    r = requests.post(url, files=files).json()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Visualizing the test image\n",
    "\n",
    "First we load the image with Pillow and display the image in our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'samples/chap4_summary.jpg'\n",
    "image = Image.open(img_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Call model to detect objects in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_response = call_model(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize model response\n",
    "\n",
    "The model returns JSON containing a `text` field which is an array of strings, one for each object detected in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the model results - there should be a `text` array\n",
    "import json\n",
    "print(json.dumps(model_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_response['status']=='ok':\n",
    "    for textgroup in model_response['text']:\n",
    "        print(' '.join(textgroup))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
